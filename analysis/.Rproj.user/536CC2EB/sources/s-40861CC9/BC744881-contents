---
title:  "Crowdsourcing Political Models of Covid-19 Mortality: A Pre-Analysis Plan"
author: "Miriam Golden^[Steering Committee Co-Chair], Alex Scacco^[Steering Committee Co-Chair],  Alberto Diaz-Cayeros, Kim Dionne, Macartan Humphreys, Sampada KC, Eugenia Nazrullaeva, Tara Slough, Eva Vivalt, and Haoyu Zhai"
date: "`r Sys.Date()`"
abstract: |
  A design for soliciting, selecting, and aggregating community-provided socio-political models predicting Covid-19 mortality. 
output: 
  bookdown::html_document2:
    number_sections: yes
    code_folding: hide
    toc: yes
    toc-depth: 2
    toc_float: yes
    theme: cosmo
always_allow_html: true
---

```{r setup, include=FALSE}

# pkgs
if (!require("pacman")) install.packages("pacman") # pkg manager

pacman::p_load(tidyverse, DeclareDesign) # generals 
pacman::p_load(kableExtra, stargazer, DT, patchwork, ggpubr) # formats
pacman::p_load(glmnet, CVXR, boot, parallel) # analytics

# knitr options
knitr::opts_chunk$set(
    echo = TRUE, cache = TRUE, # incl code & use cache
    message = FALSE, warning = FALSE # turn off messages/warnings
  ) 
options(tidy.opts=list(width.cutoff=60),
        tidy=TRUE, # wrap code
        DT.options = list(pageLength = 5, scroller = TRUE, scrollX = TRUE)) # table page width

```

```{r helper}

# print fns
pretty_print <- function(fun){
    captured <- capture.output(fun)
    captured[1] <- paste(as.character(substitute(fun)), "<-", captured[1])
    cat(paste(captured, collapse="\n"))
}

# kable layout
style_kable <- function(kbl) {
  kbl %>% kable_paper(bootstrap_options = "striped", full_width = T)
} 


```

# Overview {.tabset .tabset-pills}

## Goal

Our goal is to aggregate the collective knowledge of social scientists to predict future cross-national and sub-national patterns of Covid-19 mortality using political and socio-economic variables.

Covid-19 has presented a common challenge to countries around the world, with governments and societies employing a range of strategies to limit the health and economic impacts of the underlying disease. One year into the pandemic, we see stark variation in mortality rates between countries and regions within countries. This variation likely reflects a combination of preexisting factors --- demographic structures, health system capacities, climatic conditions --- and choices made by governments and citizens regarding how to respond to the threat of the pandemic. 

Our project *gathers*, *selects*, and *aggregates* statistical models of Covid-19 mortality. Proposed by social scientists, the models highlight the possible contributions of social and political variables to cumulative COVID-19 deaths. 

After gathering these statistical models, we elicit expert *forecasts* about the predictive power of the models from social scientists and public health experts. These forecasts contribute to our efforts to select and aggregate models. Moreover, we can also learn about expert beliefs from analysis of these forecasts. 


## Gathering {.tabset}

In December 2020, we launched an open call soliciting statistical models from political and other social scientists asking them to predict cumulative numbers of COVID-19 deaths as of 31 August 2021. Individuals or teams were encouraged to submit models to a website  showing the cumulative number of COVID-19 deaths as of 16 November 2020 as well as data we had assembled on many possible predictors, including measures of state capacity, political priorities, political institutions, and social structures. Submission of additional predictors was also possible. The interface let users provide models to predict mortality across countries (global challenge) or across states (national challenges)  in India, Mexico, and the United States. 

The platform was open to all researchers (and non-researchers). We advertised through social media (Twitter),  via professional listservs (APSA, EPSA, Polmeth, EGAP, others), and also by individual emails directed at a list of researchers from  the top 100 research institutions globally as well as specifically in the US, Mexico, and India.  

Screenshots of the  interface are given in Appendix \@ref(platform). The interface permits researchers to do the following:

  #. Choose a model challenge to enter --- Global, India, Mexico, or US.
  
  #. Select up to three predictors and see the performance of a linear bivariate model that uses each predictor on current (16 November 2020) data.
  
  #. Optionally upload new regressors not already available in our data repository.
  
  #. Optionally  change functional form of the models to allow interaction, polynomial, or custom model submissions. 
  
  #. Optionally predict parameter values for models, enabling submission of  "parameterized models."
  
  #. Provide a logic to explain the model (required). We encouraged researchers to describe why the set of predictors they chose matters for the outcome, with references to relevant literatures.
  
  #. Submit models.

As participants developed their models, they could explore how their models performed on “current data” (cumulative mortality counts up to 16 November 2020). They could also examine bivariate plots representing the relationship between each of their chosen predictors and the outcome variable (logged cumulative deaths per million). Also available were codebooks with details of variable definitions and data sources for all the  predictors.

Contributors of the ten models that receive the most weights from a stacking exercise in each challenge (described below) will be invited to be co-authors on the eventual paper reporting results. All those submitting a "legible" model will be publicity acknowledged, where "legible" means a model that meets minimal performance criteria and whose author(s) provide a clearly specified underlying logic for the choices of regressors.

### Submitted models

In total, we received 90 distinct model submissions. Table \@ref(tab:model-user) shows the submissions for each of the four challenges (Global, India, Mexico, and the US) disaggregated by model type. For ten of the models submitted, participants also uploaded their own data.      


```{r model-user, results='asis'}

# read-in models
models_submitted <- read.csv("../3_model_legibility/submitted_models_clean.csv") %>% 
  dplyr::select(df, general_model_form, specific_model_form, custom_func_spe, custom_func_general, prediction_model) %>% 
  group_by(df) %>% 
  summarise(
            General = sum(!is.na(general_model_form)|!is.na(custom_func_general)),
            Parameterized = sum(!is.na(specific_model_form)|!is.na(custom_func_spe))
            ) %>% # count gen/specs
  ungroup()
  
# table 
models_submitted %>%
  mutate(N = rowSums(models_submitted[-1])) %>%
  janitor::adorn_totals("row") %>%
  kable(format = "html", caption = "Types of model submitted", col.names = c("Data", "General", "Parameterized", "Total"), booktabs = T, escape = F) %>% 
  column_spec(2:4, width = "10em") %>% 
  style_kable()

```

<a href="#overview">Back to top</a>

### Additional models

In addition to user-submitted models, we include two other predictive models for each challenge: a model with standard epidemiological predictors ("usual suspects model") and a model with predictors selected by Lasso ("Lasso model"). 


```{r models-control}

# read-in models

controls <- read.csv("../2_output/lasso_suspects_results.csv", header = TRUE, check.names = TRUE)

# clean models

controls <- 
  filter(controls, df=="Global") %>% 
  mutate_at(vars(matches("model")), 
            ~{str_replace_all(.x, "urban_pct", "urban") %>% # urban_pct -> urban 
                str_replace_all(., "pop_density", "pop_density_log")}) %>% # pop_density -> pop_density_log 
  rbind.data.frame(filter(controls, df!="Global")) %>% 
  dplyr::select(-X, -selection) %>% 
  transform(specific_model_form = str_replace_all(specific_model_form, fixed("*"), "\\*")) %>% # escape *
  transform(general_model_form = str_replace_all(general_model_form, fixed("+"), " + ")) %>% # add space to + (for linebreaks)
  transform(specific_model_form = str_replace_all(specific_model_form, fixed("+"), " + ")) %>% 
  transform(specific_model_form = paste0("deaths_per_mio_log ~<br>", specific_model_form)) %>% # add dv to spec-mod
  transform(general_model_form = str_replace_all(general_model_form, "deaths_per_mio_log ~ ", "deaths_per_mio_log ~<br>")) # add linebreak to gen-mod dv

```

We include a usual suspects model containing standard epidemiological predictors in each challenge to assess the additional explanatory power of social and political variables beyond basic epidemiological predictions. 

The usual suspects models are given below:

```{r model-control-epi}

controls %>% 
  filter(str_detect(entrant, "suspects")) %>% 
  dplyr::select(-entrant) %>% 
  kable(caption = "Usual suspects models", col.names = c("Data", "General Form", "Parameterized Form"), escape = F) %>% 
  kable_styling() %>% 
  column_spec(1, width = "5em") %>% 
  column_spec(2:3, width = "15em") %>% 
  style_kable()

```

The Lasso models are generated by implementing Lasso on all challenge-provided variables to select a model with 2--5 predictors for each challenge. To remain faithful to the prediction challenge, the Lasso models use cumulative deaths per million (logged) as of 21 November, 2020.

To address missingness in predictors and outcomes, we impute the mean of any variable (outcome or predictor) for any missing value in the dataset prior to implementing Lasso. One issue that emerged in implementing the Lasso models was that different partitions of the training/testing sets yield different models. To address this issue, we identify the most common 2--5 predictor model over 500 iterations of Lasso for each challenge. 

Our Lasso models are described below:

```{r model-control-lasso}

controls %>% 
  filter(str_detect(entrant, "lasso")) %>% 
  dplyr::select(-entrant) %>% 
  kable(caption = "Lasso models", col.names = c("Data", "General Form", "Parameterized Form"), escape = F) %>%
  kable_styling() %>% 
  column_spec(1, width = "5em") %>% 
  column_spec(2:3, width = "15em") %>% 
  style_kable()

```

<a href="#overview">Back to top</a>

### Summary of model types

Our model-gathering procedure results in four types of models: "theory-driven" models that prespecify predictors (the typical case) and Machine Learning (custom) models that use automated processes to select predictors, each coming, possibly, in a "general" form --- in which parameter values are to be estimated using future data --- and in a "parameterized" form --- in which parameter values are provided with the model at the time of submission. Table \@ref(tab:model-summary) shows all models disaggregated by the four types. (Note: we treat Lasso models as Machine Learning models.)

```{r model-summary}

# all models
models_all <- read.csv("../3_model_legibility/submitted_models_clean.csv") %>% 
  dplyr::select(df, general_model_form, specific_model_form, custom_func_spe, custom_func_general, prediction_model) %>% 
  rbind.data.frame(controls %>% # add type cols
                     mutate(custom_func_spe = NA, custom_func_general = NA,
                            prediction_model = if_else(str_detect(entrant, "lasso"), 1, 0)) %>% 
                     select(-entrant)) %>% 
  mutate(type = if_else(prediction_model==1, "ML", "Theoretical")) %>% # add types
  transform(type = fct_relevel(type, "Theoretical")) %>% # set theoreticals as base (left col)
  group_by(df, type) %>% # count by df-type
  summarise(
            General = sum(!is.na(general_model_form)|!is.na(custom_func_general)),
            Parameterized = sum(!is.na(specific_model_form)|!is.na(custom_func_spe))
            ) %>% # count gen/specs
  ungroup() %>% 
  pivot_wider(names_from = type, values_from = c("General", "Parameterized"), values_fill = 0) %>% # split by type again
  relocate(General_ML, .after = Parameterized_Theoretical) # set cols order (theo, ml)

# table 
models_all %>%
  mutate(N = rowSums(models_all[-1])) %>%
  janitor::adorn_totals("row") %>%
  kable(format = "html", caption = "Types of all models", booktabs = TRUE, escape = FALSE, col.names = c("Data", rep(c("General", "Parameterized"), 2), "Total")) %>% 
  add_header_above(header = c(" " = 1, "Theoretical" = 2, "Machine Learning" = 2, " " = 1)) %>% 
  column_spec(2:5, width = "10em") %>% 
  style_kable()

```
<a href="#overview">Back to top</a>

## Selecting {.tabset}

We compare four approaches to selecting among models. In all cases we rely on a form of out-of-sample prediction to assess the value of a model. We describe this feature next.

### Out-of-sample prediction

Models are selected on the basis of their ability to aid in prediction of future Covid-19 mortality.

For Machine Learning submissions, predictor selection is implemented using data available at the time of submission.

For all model types, we seek a form of out-of-sample prediction. Table \@ref(tab:select-types) summarizes.

```{r select-types}  

prediction_types <- 
data.frame(
Type = c("Theoretical Model", "ML model"),
Description = c("In time $s$, models specify a logic, a set of variables,  a functional form, and, optionally, a set of parameter values.",
"In time $s$, models specify an algorithm that identifies a set of variables and a functional form and calculates a set of parameter values"),
General = c("Model parameters are re-estimated for each unit $i$ on time $t$ data, excluding $i$, and predictions are then generated for $i$. This produces a complete leave-one-out prediction vector.", "The search for variables or functional forms is not repeated at time $t$. Model parameters are re-estimated for each unit $i$ on time $s$  data, excluding $i$, and predictions are then generated for $i$. This produces a complete leave-one-out prediction vector."),
Parameterized = c("Submitter-provided parameters in time $s$ are used to generate predictions (using fixed covariate data).",
"Parameters values estimated in time $s$ are used to generate predictions for time $t$  (using fixed covariate data) unless alternative user provided values are provided.")
)

kable(prediction_types, format = "html", caption = "Types of predictions.", booktabs = TRUE, escape = FALSE) %>%
  column_spec(1, width = "8em") %>%
  column_spec(2:4, width = "15em") %>% 
  style_kable() 

```           

For the parameterized models, we naturally have a form of out-of-sample prediction since we use fixed  models to  predict future outcomes.  For general models we emulate out-of-sample prediction --- and thereby guard against overfitting --- by focusing on the leave-one-out predictions (LOO) of each model.

Given predictor variable(s), $x$, and model $k$ with parameters $\theta_k$, the predicted value of $y$ can be written: 

$$\widehat{y}_k = f_k(x|\theta_k)$$

Parameter vector $\theta_k$ is either generated from data (for "general models") or provided by researchers ("parameterized models"). 

In the case of general models, we let $\theta_k^{-i}$ denote the parameter values generated when data on unit $i$ is not available.

The leave-one-out prediction for unit $i$ is then:

$$\widehat{y}^\text{loo}_{ik} =f_k(x|\theta^{-i}_k)$$
The predicted profile is given by:

$$\widehat{y}_k :=\left(\widehat{y}^\text{loo}_{ik}\right)_{i\in 1\dots N}$$

For the specific (parameterized) models, $\theta$ is provided by researchers and so the leave-one-out predictions ($\widehat{y}$) are simply the predicted values.

Model success is defined for all model types as:

$$v_k := -\sum_i\left(\widehat{y}_{ik} - y_i\right)^2$$
**Handling missingness**: Note that in the absence of a user-specified imputation, imputation-procedure predictions will be missing for any observation that is missing data on any predictor. In such cases, we use the  mean of all predictions provided to impute predictions for units with missing values. In other words: if a submitter's model fails to provide a prediction for a unit, the unit will be assigned the average prediction. With user-specified imputation, we impute predictors according to their specifications and then fit the LOO model. 

<a href="#overview">Back to top</a>

### Selection strategies 

Besides selecting for "legibility", we employ four  strategies for selecting among proposed models:

  1. **Variation explained** $(v_k)$: We will calculate the "leave-one-out" predictions for each model and use this to calculate residual squared error.

  2. **Expected top performer** $(\hat{v}_k)$: We use a forecasting platform to elicit expert expectations of the success of models in explaining residual variation. We describe the forecasting approach in more detail in the next section.

  3. **Contribution to aggregation** $(w_k)$: We use a stacking approach (described in the next section) to assess the weight placed on each model when forming an aggregate model. 

  4. **Expected contribution to aggregation** $(\hat{w}_k)$: We use a forecasting platform to assess the *average* weight that experts place on each  model when forming an aggregate model.
  
For all four strategies we will focus the top five models. For strategies 1 and 3, we calculate standard errors and 95\% confidence intervals using nonparametric bootstraps with 1000 iterations. For 2 and 4, we calculate the SEs and CIs by the variance of weights across respondents.

<a href="#overview">Back to top</a>

### Detail on stacking weights

We use "model stacking" to aggregate the predictions of all user models, the usual suspects models, and the Lasso-selected models. The stacking approach estimates a set of weights $w$ so that the weighted average of model predictions has the smallest possible error. 

Formally we estimate:

$$
\begin{align}
w := \text{arg}\min_w \sum_{i = 1}^n \left(y_i - \sum_{k} w_k \widehat{y}^\text{loo}_{ik}\right)^2 & \text{s.t. } w_k \geq 0, \sum_{k=1}^K w_k = 1 \label{eq:w}
\end{align}
$$

Larger weights provide a measure of the contribution of a model to an aggregate model and are taken here as a measure of unique predictive ability *within the set of models provided*. 

We implement the optimization using the following code:

```{r select-detail-stack}

stack_model <- function(y, XX) {
  w           <- Variable(ncol(XX))
  objective   <- Maximize(-sum(({XX %*% w} - y)^2))
  constraints <- list(w >= 0, sum(w) == 1)
  prob        <- Problem(objective, constraints)
  result      <- solve(prob)
  weights     <-  result$getValue(w)
  rownames(weights) <- colnames(XX)
  weights
}

pretty_print(stack_model)

```

In practice, the procedure is stochastic and generates a distribution over $w$.

These weights are calculated for all four challenges and for each of the four model types.

<a href="#overview">Back to top</a>

### Details on forecast elicitation

Using the Social Science Prediction Platform, we will request political and social scientists forecast the performance of the models submitted to the model challenge in Stage One.

Each respondent will be presented with a set of general models. They will be asked to either provide weights for a stacking model (&#8220;stacking weights forecasts&#8221;) or provide probabilities representing the likelihoods they think that model will perform the best in the set when combined with a set of controls (&#8220;horse race forecasts&#8221;). As forecasting stacking weights may be cognitively challenging, it is possible that researchers will perform better at estimating the likelihood a model performs the best than the expected contribution to aggregation. Each respondent will be asked to provide one set of global model forecasts and at least one set of within-country model forecasts. The forecasts will be unincentivized.

|                          | General models |
|--------------------------|----------------|
| Provide stacking weights | 50%            |
| Predict error            | 50%            |

For all forecasting analyses, we will restrict attention to those individuals who take at least 3 minutes in total on the entire forecast, on the assumption that those who took less time may not have put effort into it. If anyone enters 0 weights for every model within a question and time period, we will exclude their responses to that question and time period.

Further, since our main interest is in expert judgments, we will focus on responses from faculty members or other researchers (including researchers outside academia, post-docs and graduate students).

<a href="#overview">Back to top</a>
  
## Aggregating {.tabset}

Beyond selecting models, we are interested in *aggregating* them and assessing whether it is possible to provide overall expectations given the insights and strengths of different models. 

We make use of three approaches, discussed in detail in each section below:

  1. **Simple stacking**.
  2. **Representative experts**.
  3. **Wisdom of crowds**.
  
### Simple stacking 

The stacking weights described above in effect let us aggregate across models to generate a new and more accurate predictive model. 

With the weights for each model, we can estimate our aggregate prediction for an observation with predictors $x$ as: 

$$
\begin{align}
\widehat{y}^*_i = \sum_{k} w_k \widehat{y}^\text{loo}_{ik}
\end{align}
$$
The aggregate predictions can be assessed in the same ways as component models are assessed:

$$v^* := -\sum_i\left(\widehat{y}_{i}^* - y_i\right)^2$$

<a href="#overview">Back to top</a>

### Representative expert

Each expert's weighting also generates an aggregate model with prediction for unit $i$ by expert $j$ of:

$$\widehat{y}^j_i = \sum_{k} \hat{w}^j_k \widehat{y}^\text{loo}_{ik}$$

and success:

$$v^j := -\sum_i\left(\widehat{y}_{i}^j - y_i\right)^2$$
The representative expert's aggregate model (set) is then given by:

$$w^r = \{\hat{w}^j\ | v^j = \text{median}(v^h)_{h \in N}\}$$ 

<a href="#overview">Back to top</a>

### Wisdom of crowds

For the "wisdom of crowds" aggregation we calculate, for each model set, the normalized average weight placed on models by experts. 

$$w^c_k := \frac{\sum_j \hat{w}_k^j}{\sum_{k}{\sum_j \hat{w}_k^j}}$$

which gives predictions and values given by:

$$\widehat{y}^c_i = \sum_{k} w^c_k \widehat{y}^\text{loo}_{ik}$$

$$v^c := -\sum_i\left(\widehat{y}_{i}^c - y_i\right)^2$$
<a href="#overview">Back to top</a>

# Analyses {.tabset .tabset-pills}

Our main analyses focus on model performance, selection among models, and model aggregation. We discuss each in turn.

## Setup

We simulate dummy data for demonstration purposes. Our dummy dataset consists of one outcome and five models for each of the four challenges (Global, India, Mexico, and the USA), each with both a general and a parameterized form. We set the number of observations to 100 and 50 for the global and each of the three national challenges, respectively. We also include a Lasso and a usual suspect model for each challenge, again with both general and parameterized forms. 

```{r data-setup}

# parameters

challenges <- c("Global", "India", "Mexico", "USA") # challenges

types <- c("General", "Parameterized") # model forms

challenge_type <- do.call(paste0, expand.grid(challenges, "-", types)) # challenge + type 

nobs <- rep(c(100, 50, 50, 50), 2) # n.obs

# lab/levels
challenge_label <- gsub("-", ", ", challenge_type)
model_level <- c(paste0("M", 1:5), "Lasso", "Epi")

```

```{r data-models}

# model data

data_aug2020 <- vector(mode = "list", length = length(challenge_type))

y_aug2020 <- vector(mode = "list", length = length(challenge_type)) # same df-y 

for (i_y in 1:(length(y_aug2020)/2)) {
  set.seed(i_y)
  i_nobs <- nobs[[i_y]]
  y_aug2020[[i_y+4]] <- y_aug2020[[i_y]] <- rnorm(n=i_nobs)
}

for (i_challenge in seq(data_aug2020)) {
  set.seed(i_challenge)
  i_nobs <- nobs[[i_challenge]]
  coefs <- runif(5, -1, 1)
  i_set <- data.frame(
    y = y_aug2020[[i_challenge]]
  ) %>% 
    mutate(M1 = coefs[[1]]*y + rnorm(i_nobs),
    M2 = coefs[[2]]*y + rnorm(i_nobs),
    M3 = coefs[[3]]*y + rnorm(i_nobs),
    M4 = coefs[[4]]*y + rnorm(i_nobs),
    M5 = coefs[[5]]*y + rnorm(i_nobs)
    ) %>% 
    mutate(
      Lasso = y + rnorm(i_nobs),
      Epi = Lasso * 0.8 + rnorm(i_nobs, 0.1, 2))
  data_aug2020[[i_challenge]] <- i_set
}
names(data_aug2020) <- challenge_type

```

## Gathering

We first *gather* our models submitted by our participants.

```{r gather-helpers}

# model fit (raw r2)
gather_r2 <- function(df, y_var = "y") {
  
  # get y
  y <- df[[y_var]]
  
  # get r2
  r2 <- map_dbl(df[, !names(df) == y_var], ~summary(lm(y~.x, data = df))$r.squared)
  
  # make df
  r2 <- as.data.frame(r2) %>% 
    rownames_to_column(var = "model")
  
  # return r2
  r2
  
}

# plot model fit
gather_plot <- function(df, r2, y_var = "y", control_var = c("Lasso", "Epi"), x_lab = "Actual Deaths", y_lab = "Predicted Deaths", p_size = 0.3, p_alpha = 0.8) {
  
  # enquo arg
  y_var <- sym(y_var)

  # make tidy df
  df <- df %>% 
    pivot_longer(-(!!y_var), names_to = "model", values_to = "y_pred") %>% 
    left_join(r2) %>% 
    transform(r2 = round(r2, 3), model = factor(model, levels = model_level)) %>% 
    mutate(model_lab = paste0(model, "\n", "R\u00b2 = ", r2)) 

  # axis lims
  xmin <- ymin <- min(pull(df, y_var))
  xmax <- ymax <- max(pull(df, y_var))
  
  # plot df
  ggplot(df, aes(!!y_var, y_pred)) +
    geom_abline(slope = 1, intercept = 0) +
    geom_point(size = p_size, alpha = p_alpha) +
    geom_smooth(method = "loess", lty = "dashed", size = 2*p_size, color = "red") +
    coord_cartesian(xlim = c(xmin, xmax), 
                    ylim = c(ymin, ymax)) +
    facet_wrap(.~model_lab) +
    labs(x = x_lab, y = y_lab) +
    theme_bw() +
    theme(strip.background = element_rect(fill = NA)) +
    ggpubr::labs_pubr()
  
}

```

We measure model performance with simple $R^2$, using the following code:

```{r gather-fit}

pretty_print(gather_r2)

gather_fit <- map(data_aug2020, gather_r2) 

```

The graph below shows the performances of the gathered models. The $R^2$ statistics are added for reference.

```{r gather-plot, fig.height = 18, fig.width = 10, fig.align = 'center', fig.cap = "Gathering models."}

plot_gather <- map2(data_aug2020, gather_fit, ~gather_plot(.x, .y))

plot_gather[[challenge_type[1]]] + ggtitle(challenge_label[[1]]) + 
  plot_gather[[challenge_type[5]]] + ggtitle(challenge_label[[5]]) +
  plot_gather[[challenge_type[2]]] + ggtitle(challenge_label[[2]]) +
  plot_gather[[challenge_type[6]]] + ggtitle(challenge_label[[6]]) +
  plot_gather[[challenge_type[3]]] + ggtitle(challenge_label[[3]]) +
  plot_gather[[challenge_type[7]]] + ggtitle(challenge_label[[7]]) +
  plot_gather[[challenge_type[4]]] + ggtitle(challenge_label[[4]]) +
  plot_gather[[challenge_type[8]]] + ggtitle(challenge_label[[8]]) +
  plot_annotation(title = "Gathering Models",
                  subtitle = "Predicted vs actual mortality rates") +
  plot_layout(ncol = 2) 

```

<a href="#analyses">Back to top</a>

## Selecting {.tabset}

We then *select* our models to learn about the about which models best predict (logged) COVID-19 deaths per million. We will assess models' ability to predict based on the four strategies set out in section \@ref(selection-strategies). We then intend to substantively interpret the best performing models in each challenge.

For demonstration purposes we also use dummy data for respondent forecasts. We set the number of respondents to 50 for the global challenge and 30 for each of the national challenges.
  
```{r select-data}

n_forecast <- rep(c(50, 30, 30, 30), 2)

# crowd r2
data_forecast_r <- vector(mode = "list", length = length(challenge_type))

for (i_select in seq(data_forecast_r)) {
  set.seed(i_select)
  i_nobs <- n_forecast[[i_select]]
  i_set <- cbind.data.frame(
    M1 = runif(i_nobs),
    M2 = runif(i_nobs),
    M3 = runif(i_nobs),
    M4 = runif(i_nobs),
    M5 = runif(i_nobs),
    Epi = runif(i_nobs) # no lasso for r2 (pw)
  ) 
  i_rsum <- rowSums(i_set)
  i_set <- i_set/i_rsum # normalise r2 (pw)
  data_forecast_r[[i_select]] <- i_set
}
names(data_forecast_r) <- challenge_type

# crowd w
data_forecast_w <- vector(mode = "list", length = length(challenge_type))

for (i_select in seq(data_forecast_w)) {
  set.seed(i_select+0.5)
  i_nobs <- n_forecast[[i_select]]
  i_set <- cbind.data.frame(
    M1 = runif(i_nobs),
    M2 = runif(i_nobs),
    M3 = runif(i_nobs),
    M4 = runif(i_nobs),
    M5 = runif(i_nobs),
    Lasso = runif(i_nobs),
    Epi = runif(i_nobs) 
  ) 
  i_rsum <- rowSums(i_set)
  i_set <- i_set/i_rsum # normalise ws
  data_forecast_w[[i_select]] <- i_set
}
names(data_forecast_w) <- challenge_type

```

```{r select-helpers-tools}

# tidy bootstrap 
tidy_boot_select <- function(df, var_names = c("model", "w_mean", "w_lwr", "w_upr")) {
  df %>% 
    pivot_longer(cols = everything(), names_to = "model", values_to = "value") %>% 
    group_by(model) %>% 
    summarise(mean = mean(value),
              lwr = quantile(value, 0.025),
              upr = quantile(value, 0.975)) %>% 
    `colnames<-`(var_names)
}

# detect cores
ncores <- parallel::detectCores() - 1

```

```{r select-helpers-main}

# residual r2

## residual r2
select_r2 <- function(df, var_y = "y", var_partial = "Lasso", out_name = "r2") {
  
  # get xx varnames 
  var_xx0 <- setdiff(colnames(df), c(var_y, var_partial))
  
  # remove any space/special-chars in colnames 
  # (skip lm parsing error)
  colnames(df) <- gsub(" |[[:punct:]]", "_", colnames(df))
  
  # get xx varnames (again, post cleaning)
  var_xx <- setdiff(colnames(df), c(var_y, var_partial))

  # set base lm formula
  model_yz <- paste0(var_y, "~", paste0(var_partial, collapse = "+"))
  
  # residualise y
  df[[var_y]] <- lm(as.formula(model_yz), data = df)$residuals
  
  # loop over x cols, residualise then get partial r^2
  n_xx <- length(var_xx)
  pr2 <- numeric(length = n_xx)
  for (i in seq(n_xx)) {
    model_yx <- as.formula(paste0(var_y, "~", var_xx[i]))
    model_xz <- as.formula(gsub("y", var_xx[i], model_yz))
    df[[var_xx[i]]] <- lm(as.formula(model_xz), data = df)$residuals
    pr2[i] <- summary(lm(model_yx, data = df))$r.squared
  }
  
  # add xx names
  r2 <- cbind.data.frame(var_xx0, pr2)
  colnames(r2) <- c("model", out_name)
  
  # return r^2
  r2
  
}

## bootstrap r2 var
select_r2_boot <- function(df, r = 1000, ranseed = 1) {
  
  # inner fn for bootstat
  bstat_r2 <- function(df, i) {
  d <- df[i,]
  r2 <- select_r2(d)
  return(r2[[2]])
}
  
  # bootstrap r2
  set.seed(ranseed)
  boot.out <- boot(data = df, statistic = bstat_r2, R = r, parallel = "multicore", ncpus = ncores)
  
  # extract df
  boot.out.df <- as.data.frame(boot.out[["t"]])
  names(boot.out.df) <- names(df[, -which(names(df) %in% c("y", "Lasso"))])
  
  # return df
  return(boot.out.df)
  
}

# stacking wts

## stack fn (also defined in select-detail-stack above)
select_stack <- function(y, XX) {
  w           <- Variable(ncol(XX))
  objective   <- Maximize(-sum(({XX %*% w} - y)^2))
  constraints <- list(w >= 0, sum(w) == 1)
  prob        <- Problem(objective, constraints)
  result      <- solve(prob)
  weights     <-  result$getValue(w)
  rownames(weights) <- colnames(XX)
  weights
}

## bootstrap stacking
select_stack_boot <- function(df, r = 1000, ranseed = 1) {
  
  # inner fn for bootstat
  bstat_stack <- function(df, i) {
  d <- df[i,]
  w <- select_stack(y = d[["y"]], XX = d[, -which(names(d) == "y")])
  return(w)
}
  
  # bootstrap w
  set.seed(ranseed)
  boot.out <- boot(data = df, statistic = bstat_stack, R = r, parallel = "multicore", ncpus = ncores)
  
  # extract df
  boot.out.df <- as.data.frame(boot.out[["t"]])
  names(boot.out.df) <- names(df[, -which(names(df) == "y")])
  
  # return df
  return(boot.out.df)
  
}


# forecast r2/w
select_forecast <- function(df, out = c("r2", "w")) {
  
  # get values
  m_crowd <- df %>%
    map_dbl(., mean) %>% 
    as.data.frame() %>% 
    rownames_to_column(var = "model") %>% 
    rename(m = 2)
  se_crowd <- df %>% 
    map_dbl(., sd) %>% 
    as.data.frame() %>% 
    rownames_to_column(var = "model") %>% 
    rename(se = 2)
  crowd <- full_join(m_crowd, se_crowd, by ="model")
  crowd_out <- crowd %>% 
    transmute(
      model = model,
      m = m,
      lwr = m - 1.96*se,
      upr = m + 1.96*se)
  
  # rename cols
  col_m <- paste0(out, "_mean")
  col_lwr <- paste0(out, "_lwr")
  col_upr <- paste0(out, "_upr")
  colnames(crowd_out) <- c("model", col_m, col_lwr, col_upr)
  
  # return df
  crowd_out
  
}


```

### Real data

For strategy 1, we measure **variance explained** ($v_k$) by Lasso-residualized $R^2$, using the following code:

```{r select-r2}

pretty_print(select_r2)

r2_select <- map(data_aug2020, select_r2_boot)

r2_select_tidy <- map(r2_select, ~tidy_boot_select(.x, var_names = c("model", "r2_mean", "r2_lwr", "r2_upr")))

```

For strategy 3, we measure **contribution to aggregation** ('stacking weights', $w_k$) by stacking weights, with the following code:

```{r select-w}

# cache to save time

pretty_print(select_stack)

w_select <- map(data_aug2020, select_stack_boot) # note the dif fn (with bootstrap)

w_select_tidy <- map(w_select, tidy_boot_select)

```

In both cases, we construct the 95\% CIs using 100 nonparametric bootstrapped subsamples.

<a href="#analyses">Back to top</a>

### Forecaster data

For strategies 2 and 4, we measure **expected top performer** ('expected variance explained', $\hat{v}_k$) and **expected contribution to aggregation** ('expected weight', $\hat{w}_k$) with the average forecast among respondents. We construct their CIs using variance in respondent forecasts. The codes we use for analyzing forecaster predictions are:

```{r select-forecast}

pretty_print(select_forecast)

r2_forecast <- map(data_forecast_r, ~select_forecast(.x, out = "r2"))
w_forecast <- map(data_forecast_w, ~select_forecast(.x, out = "w"))

```

<a href="#analyses">Back to top</a>

### Graphical illustration

The graph below shows the performances of the models across selecting strategies. In actual analysis, we will select the top 5 performers under each strategy. 

```{r select-plot, fig.height = 9, fig.width = 10, fig.align = 'center', fig.cap = "Selecting models."}

# stack dfs
r2_both <- rbind.data.frame(
  bind_rows(r2_select_tidy, .id = "df") %>% 
    rename_with(~str_remove_all(.x, "r2_"), starts_with("r2_")) %>% 
    mutate(metric = "Variance explained"),
  bind_rows(r2_forecast, .id = "df") %>% 
    rename_with(~str_remove_all(.x, "r2_"), starts_with("r2_")) %>% 
    mutate(metric = "Expected var explained")
)
w_both <- rbind.data.frame(
  bind_rows(w_select_tidy, .id = "df") %>% 
  rename_with(~str_remove_all(.x, "w_"), starts_with("w_")) %>% 
  mutate(metric = "Stacking weight"),
bind_rows(w_forecast, .id = "df") %>% 
  rename_with(~str_remove_all(.x, "w_"), starts_with("w_")) %>% 
  mutate(metric = "Expected weight")
)

# bind dfs
select_all <- rbind.data.frame(r2_both, w_both)

# split cols
select_all <- select_all %>% 
  separate(col = df, into = c("challenge", "type"), sep = "-") 

# reorder cols
select_all <- select_all %>% 
  transform(model = factor(model, levels = model_level)) %>% 
  transform(metric = factor(metric, levels = c("Variance explained", "Expected var explained", "Stacking weight", "Expected weight"))) %>% 
  transform(type = factor(type, levels = c("General", "Parameterized")))

# reorder df (bug; not run)
#select_all <- select_all %>% 
#  group_by(challenge) %>% 
#  arrange(mean, metric) %>% 
#  mutate(order = row_number())

# plot df
p_dodge <- position_dodge(0.7)
ggplot(select_all, aes(x = model, y = mean, color = type)) +
  geom_point(position = p_dodge) +
  geom_errorbar(aes(ymin = lwr, ymax = upr), position = p_dodge, width = 0.6) +
  facet_grid(challenge~metric, scales = "free_x") +
  theme_bw() + scale_color_grey(start = 0.2, end = 0.6) +
  labs(title = "Selecting Models", subtitle = "Actual vs expected variance/weight",
       x = NULL, y = NULL, color = "Model Type") +
  theme(strip.background = element_rect(fill = NA),
        axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5)) +
  ggpubr::labs_pubr()

```

<a href="#analyses">Back to top</a>

### Interpretation

For each challenge, we will examine (qualitatively) the features of the two top-performing models. Because of the four selection strategies, it is possible that the top two models are not the same across all selection strategies (columns in \@ref(select-plot)). In these cases, we will select the two models that *most frequently* appear in the top two (across the selection strategies). In the case of ties, we will examine all tied models. 

Our efforts to understand the characteristics of successful models will be exploratory. To this end, we will consider questions like: does theory help in picking such models; what type(s) of predictors best explain the outcome; do we observe evidence of the submitted logic behind the model, etc.

<a href="#analyses">Back to top</a>

## Aggregating {.tabset}

Finally we *aggregate* our models using three main (see sec.\@ref(aggregating)) and three additional approaches. Our primary analysis focuses on the relative success of aggregation strategies to predict mortality levels in August 2021. In addition we assess:

  * **Score approach**: we normalize model success estimates across all approaches, and compare model performances using this metric.
  
  * **Given 2020 data**: we calculate the *change* from 2020 to 2021 data and compare model performances on this first-difference outcome.
  
  * **Longer term prediction**: we compare model performances directly on 2021 data.
  
```{r aggregate-data}

# new y
data_aug2021 <- data_aug2020

for (i_challenge in seq(data_aug2021)) {
  i_challenge.seed <- ifelse(i_challenge > 4, i_challenge - 4, i_challenge) # same df-dy
  set.seed(i_challenge.seed)
  i_growth <- rnorm(nrow(data_aug2021[[i_challenge]]), 0.05, 0.1)
  data_aug2021[[i_challenge]][["y"]] <- 0.5*data_aug2020[[i_challenge]][["y"]] + i_growth
}

names(data_aug2021) <- challenge_type

# delta_y
data_aug2020_2021 <- data_aug2021
for (i_challenge in seq(data_aug2020_2021)) {
  data_aug2020_2021[[i_challenge]][["y"]] <- data_aug2021[[i_challenge]][["y"]] - data_aug2020[[i_challenge]][["y"]] # dif(y)
}

names(data_aug2020_2021) <- challenge_type


```

```{r aggregate-helpers-success}

# model success
eval_success <- function(var_y = y, var_yhat = y_hat) {
  -sum((var_y - var_yhat)^2)
}
eval_base <- function(var_y = y) {
  -sum((var_y - mean(var_y))^2)
}

```

```{r aggregate-helpers-tools}

# model ids
get_models <- function(df_mods, id_y = "y") {
  id_mods <- colnames(df_mods)
  id_mods[id_mods != id_y]
}

# wts 
get_ws <- function(df_w, id_w = "w_mean", id_w_lwr = "w_lwr", id_w_upr = "w_upr") {
  df_w[, c(id_w, id_w_lwr, id_w_upr)]
}

# tidy boot (agg)
tidy_boot_agg <- function(df) {
  
  # extract df
  df.out <- df[["t"]]

  # get estimates
  data.frame(success = mean(df.out),
             success_lwr = quantile(df.out, 0.025),
             success_upr = quantile(df.out, 0.975)) %>% 
    `rownames<-`(NULL)
  
}

```

```{r aggregate-helpers-main}

# stacking

## stacking
agg_sumstack <- function(df_w, df_mods) { 
  
  # get model ids
  id_mods <- get_models(df_mods)
  
  # apply wts
  ws <- get_ws(df_w)
  df_yhat <- df_mods[, id_mods]
  df_pred <- as.matrix(df_yhat) %*% as.matrix(ws[[1]]) 

  # sum wtd yhats
  y_hat <- apply(df_pred, 1, sum)
  
  # eval success
  success <- eval_success(var_y = df_mods[["y"]], var_yhat = y_hat)

  # return dfs
  data.frame(success)
  
}

## bootstrap stacking 
agg_sumstack_boot <- function(df_w, df_mods, r = 1000, ranseed = 1) { # note: df_w = raw stack wts from bootstrap
  
  # inner fn for bootstat
  bstat_sumstack <- function(df_w, i) {
    d_w <- df_w[i,]
    d_w <- tidy_boot_select(d_w)
    success <- agg_sumstack(d_w, df_mods = df_mods)
    return(success[[1]])
  }
  
  # boot ws
  set.seed(1)
  boot.out <- boot(data = df_w, statistic = bstat_sumstack, R = r, parallel = "multicore", ncpus = ncores)
  
  # return sum stat
  tidy_boot_agg(boot.out)
  
}


# representative expert

## representative expert
agg_represent <- function(df_w_raw, df_mods, id_y = "y", id_w = "w_mean", id_w_lwr = "w_lwr", id_w_upr = "w_upr") { # note: df_w_raw = forecaster df (row = ind. forecaster)
  
  # get model ids
  id_mods <- get_models(df_mods)
  
  # get y
  y <- df_mods[[id_y]]
  
  # loop to apply wts (each)
  ls_mod <- vector(mode = "list", length = length(id_mods))
  names(ls_mod) <- id_mods
  for (id_mod in id_mods) {
    w <- df_w_raw[[id_mod]]
    m <- df_mods[[id_mod]]
    ls_rss <- ls_w <- vector(mode = "list", length = length(w))
    for (i_w in seq(w)) {
      m_wi <- m*w[i_w]
      ls_w[[i_w]] <- w[i_w]
      rss_wi <- -sum((y-m_wi)^2)
      ls_rss[[i_w]] <- rss_wi
    }
    ls_rss <- unlist(ls_rss)
    rss_med <- quantile(ls_rss, probs = 0.5, type = 1)
    i_med <- which.min(abs(ls_rss - rss_med))
    w_med <- ls_w[[i_med]]
    ls_mod[[id_mod]] <- w_med
  }
  
  # loop to apply wts (rep. ws)
  df_out <- df_mods[, id_mods]
  for (id_mod in id_mods) {
    df_out[[id_mod]] <- df_mods[[id_mod]] * ls_mod[[id_mod]]
  }
  
  # sum and eval wtd yhats
  df_pred <- apply(df_out, 1, sum)
  success <- eval_success(var_y = y, var_yhat = df_pred)

  # return df
  data.frame(success)
  
}

# bootstrap rep. expert
agg_represent_boot <- function(df_w_raw, df_mods, r = 1000, ranseed = 1) {
  
  # inner fn for bootstat
  bstat_represent <- function(df_w_raw, i) {
    d_w_raw <- df_w_raw[i,]
    success <- agg_represent(d_w_raw, df_mods = df_mods)
    return(success[[1]])
  }
  
  # bootstrap rep. success
  set.seed(ranseed)
  boot.out <- boot(data = df_w_raw, statistic = bstat_represent, R = r, parallel = "multicore", ncpus = ncores)
  
  # return sum stats
  tidy_boot_agg(boot.out)
  
}


# crowd wisdom

## crowd wisdom
agg_crowd <- function(df_w, df_mods, id_w = "w_mean") {
  
  # get model ids
  id_mods <- colnames(df_mods)
  id_mods <- id_mods[id_mods != "y"]
  
  # normalize wts
  id_w_std <- paste0(id_w, "_std")
  df_w[[id_w_std]] <- df_w[[id_w]]/sum(df_w[[id_w]])
  
  # loop to apply wts
  df_out <- df_mods[, -which(names(df_mods)=="y")]
  for (id_mod in id_mods) {
    df_out[[id_mod]] <- df_mods[[id_mod]] * as.numeric(df_w[which(df_w[["model"]]==id_mod), id_w_std])
  }
  
  # sum and eval wtd yhats
  df_pred <- apply(df_out, 1, sum)
  success <- eval_success(var_y = df_mods[["y"]], var_yhat = df_pred)

  # return df
  data.frame(success)
  
}

## bootstrap wisdom
agg_crowd_boot <- function(df_w_raw, df_mods, r = 1000, ranseed = 1) { # note: use forecaster df
  
  # inner fn for bootstat
  bstat_widsom <- function(df_w_raw, i) {
    d_w_raw <- df_w_raw[i,]
    d_w_raw <- select_forecast(d_w_raw, out = "w")
    success <- agg_crowd(d_w_raw, df_mods = df_mods)
    return(success[[1]])
  }
  
  # bootstrap success
  set.seed(ranseed)
  boot.out <- boot(data = df_w_raw, statistic = bstat_widsom, R = r, parallel = "multicore", ncpus = ncores)
  
  # return sum stats
  tidy_boot_agg(boot.out)
  
}


```


```{r aggregate-helpers-extra}

# best/med single

## pick model
agg_pick <- function(df_mod, id_y = "y", metric = c("single", "median")) {
  
  # get model ids
  id_mods <- get_models(df_mod)
  
  # make model df
  df_pred <- df_mod[, id_mods]
  
  # loop to get rss
  for (id_mod in id_mods) {
    df_pred[[id_mod]] <- eval_success(df_mod[[id_y]], df_mod[[id_mod]])
  }
  df_pred <- apply(df_pred, 2, unique)
  
  # pick by metric
  if (metric=="single") {
    df_pick <- df_pred[df_pred == max(df_pred)]
  } 
  if (metric == "median") {
    df_pick <- df_pred[df_pred == median(df_pred)]
    }
  
  # make output df
  df_pick_id <- names(df_pick)
  df_pick_pred <- df_mod[[df_pick_id]]
  df_pick_succ <- eval_success(var_y = df_mod[["y"]], var_yhat = df_pick_pred) 
  df_pick <- cbind.data.frame(df_pick_id, df_pick_succ)
  colnames(df_pick) <- c("model", "success")
  rownames(df_pick) <- NULL
  
  # return df
  df_pick

}

## bootstrap picks
agg_pick_boot <- function(df_mods, metric = c("single", "median"), r = 1000, ranseed = 1) {
  
  # inner fn for bootstat
  bstat_pick <- function(df_mods, i) {
    d_mods <- df_mods[i,]
    mod <- agg_pick(d_mods, metric = metric)
    return(mod[[2]])
  }
  
  # boot picks
  set.seed(1)
  boot.out <- boot(data = df_mods, statistic = bstat_pick, R = r, parallel = "multicore", ncpus = ncores)
  
  # return sum stats
  tidy_boot_agg(boot.out)
  
}

# expert favored

## expert favored
agg_favor <- function(df_w, df_mod, id_w = "w_mean") {
  df_w <- df_w[which(df_w[[id_w]] == max(df_w[[id_w]])), c("model", id_w)]
  m_id <- df_w[["model"]]
  m_success <- eval_success(df_mod[["y"]], df_mod[[m_id]])
  data.frame(model = m_id, y_hat = m_success)
}

## bootstrap favored
agg_favor_boot <- function(df_w_raw, df_mod, r = 1000, ranseed = 1) {
  
  # inner fn for bootstat
  bstat_favor <- function(df_w_raw, i) {
    d_w_raw <- df_w_raw[i,]
    d_w <- select_forecast(d_w_raw, out = "w")
    df_favor <- agg_favor(d_w, df_mod = df_mod)
    df_favor_out <- df_favor[[2]]
    names(df_favor_out) <- df_favor[[1]]
    df_favor_out
  }
  
  set.seed(ranseed)
  boot.out <- boot(data = df_w_raw, statistic = bstat_favor, R = r, parallel = "multicore", ncpus = ncores)
  
  # return sum stats
  tidy_boot_agg(boot.out)
  
}


```

### Main approaches

For approach 1 (**stacking weights**), we use the following code to calculate model predictions:

```{r aggregate-stack}

pretty_print(agg_sumstack)

```


For approach 2 (**representative expert**), we use the following code:

```{r aggregate-represent}

pretty_print(agg_represent)

```


For approach 3 (**wisdom of crowds**), we implement it with the following code:

```{r aggregate-wisdom}

pretty_print(agg_crowd)

```

<a href="#analyses">Back to top</a>

### Additional approaches

In addition, we also add three other aggregating approaches, **best single**, **median single**, and **expert favored**, which are the single model with *the greatest success* and *the median success* on real-world data, and *the highest average weight given by forecasters*, respectively. (Success is measured in the same way as before.) We implement these with the following codes:

```{r aggregate-extra-single}

pretty_print(agg_pick)

```

```{r aggregate-extra-favor}

pretty_print(agg_favor)

```

<a href="#analyses">Back to top</a>

### Measuring success

For all approaches, we evaluate their *absolute* success by the following code:

```{r aggregate-success}

pretty_print(eval_success)

# labels
names_agg <- c("Best single", "Median single", "Stacking", "Expert favored", "Rep. expert", "Wisdom of crowds")
names_version <- c("Level", "Scores", "Given 2020 data", "2021 prediction")

```

In actual analysis, we compare such success against the outcome when no aggregation is used, i.e. the (negative) total variation. This is estimated with the following function:

```{r aggregate-success-base}

pretty_print(eval_base)

```

We measure *relative* success as the $\%$ gain from each aggregating approach over this baseline. Formally we use the following formula to calculate the relative success measure:

$$v^* := 1 - \frac{-\sum_i\left(\widehat{y}_{i}^* - y_i\right)^2}{-\sum_i\left(\bar{y} - y_i\right)^2}$$

with larger (smaller) values indicating greater (less) success from model aggregation over no action at all. We implement this measure with the following code:

```{r aggregate-success-rel}

# note: not run, for demo only
eval_relative <- function(var_y = y, var_yhat = y_hat) {
  1 - eval_success(y, y_hat) / eval_base(y)
}

pretty_print(eval_relative)

```

In practice this is equivalent to an $R^2$ statistic, where the success measures for the baseline and the assessed approach are the negative total and residual sum of squares ($TSS$ and $RSS$), respectively^[To see the equivalence, notice that as both the numerator and the denominator are the additive inverses of the $RSS$ and the $TSS$, the two negative signs cancel each other out in the resulting ratio.], and the theoretical range of this measure is 0&mdash;1. Note that since we are using simulated data for demonstration purposes, this might not be the case in the graphical output below^[Recall from the &#8220;Setup&#8221; section above that we simulated model data as scale-location transformations of a normally distributed outcome variable. The scale parameters are random draws from a [-1,1] uniform distribution, and the location (shift) parameters the standard normal distribution, except the control models.]. 

<a href="#analyses">Back to top</a>

### Graphical illustration

The graph below shows the performances of the models across aggregating strategies. The relative success measure may exceed the 0&mdash;1 theoretical range for the reasons just explained. In addition, we would not observe the general-purpose models to perform worse than the baseline on actual data, given their inclusion of an intercept term. 


```{r aggregate-plot-helpers}

# helper - add base (0)
agg_add_base <- function(df) {
  df %>% 
    add_row(approach = rep("None", 8), 
            df = rep(challenges, 2), 
            type = rep(types, each = 4), 
            success_rel = 0) %>% 
  transform(approach = factor(approach, levels = c("None", names_agg)))
}


# helper - bind to tibble
agg_to_tibble <- function(list) {
  list %>% 
    map(., ~{bind_rows(.x, .id = "df_type") %>%
           separate(col = df_type, into = c("df", "type"))} ) %>% 
    bind_rows(.id = "approach") %>% 
    transform(approach = factor(approach, levels = names_agg),
              type = factor(type, levels = c("General", "Parameterized")))
}


# wrapper - pred -> plot flow
agg_success <- function(df_w, df_w_raw, df_data, names_approach = names_agg, names_version = "Level") { 
  
  # get estimates
  ls_sumstack <- map2(df_w, df_data, ~agg_sumstack_boot(df_w = .x, df_mods = .y))
  ls_represents <- map2(df_w_raw, df_data, ~agg_represent_boot(df_w = .x, df_mods = .y))
  ls_wisdom <- map2(df_w_raw, df_data, ~agg_crowd_boot(df_w = .x, df_mods = .y))
  ls_pick_best <- map(df_data, ~agg_pick_boot(df_mod = .x, metric = "single"))
  ls_pick_median <- map(df_data, ~agg_pick_boot(df_mod = .x, metric = "median"))
  ls_favor <- map2(df_w_raw, df_data, ~agg_favor_boot(df_w = .x, df_mod = .y))
  
  # get success
  ls_all <- list(ls_pick_best, ls_pick_median, ls_sumstack, ls_favor, ls_represents, ls_wisdom)
  names(ls_all) <- names_approach
  ls_base <- map(df_data, ~eval_base(var_y = .x$y))
  
  # format to tidy
  
  ## tidy success
  ls_all_tidy <- agg_to_tibble(ls_all)
  
  ## tidy base
  ls_base_tidy <- ls_base %>% 
    bind_rows() %>% 
    pivot_longer(cols = everything(), names_to = "df", values_to = "success_base") %>% 
    transform(df = gsub("-.*", "", df)) %>% 
    distinct()
  
  # tidy relative success 
  ls_all_out <- ls_all_tidy %>% 
    left_join(ls_base_tidy) %>% 
    mutate(
      success_rel = 1 - success/success_base,
      success_lwr_rel = 1 - success_lwr/success_base,
      success_upr_rel = 1 - success_upr/success_base
    ) %>% 
    agg_add_base() %>% 
    add_column(version = names_version)
  
  # return df
  ls_all_out
  
}


```

```{r aggregate-plot-level}

agg_all_level <- agg_success(df_w = w_select, df_w_raw = data_forecast_w, df_data = data_aug2020, names_version = "Level")
  
```

```{r aggregate-plot-score}

# std succs
success_mean <- mean(agg_all_level$success_rel, na.rm = T) # avg succ
success_sd <- sd(agg_all_level$success_rel, na.rm = T) # sd succ
agg_all_score <- agg_all_level %>% 
  filter(approach != "None") %>% # no std for base
  mutate(across(.cols = ends_with("_rel"), .fns = ~{(.x - success_mean)/success_sd})) %>% 
  agg_add_base() %>% 
  transform(version = "Scores")

```

```{r aggregate-plot-given2020}

agg_all_given2020 <- agg_success(df_w = w_select, df_w_raw = data_forecast_w, df_data = data_aug2020_2021, names_version = "Given 2020 data")

```

```{r aggregate-plot-pred2021}

agg_all_2021 <- agg_success(df_w = w_select, df_w_raw = data_forecast_w, df_data = data_aug2021, names_version = "2021 prediction")

```

```{r aggregate-plot, fig.height = 9, fig.width = 10, fig.align = 'center', fig.cap = "Aggregating models. Showing success estimates between -2 and 2 only."}

# stack dfs
agg_all_all <- rbind.data.frame(
  agg_all_level,
  agg_all_score,
  agg_all_given2020,
  agg_all_2021
) %>% 
  transform(version = factor(version, levels = names_version))

# plot
ggplot(agg_all_all, aes(x = approach, color = type)) +
  geom_point(aes(y = success_rel), size = 1.2, position = p_dodge) +
  geom_errorbar(aes(ymin = success_lwr_rel, ymax = success_upr_rel), position = p_dodge) +
  facet_grid(df~version) + theme_bw() +
  scale_color_grey(start = 0.2, end = 0.6) +
  coord_cartesian(ylim = c(-2, 2)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        strip.background = element_rect(fill = NA)) +
  ggpubr::labs_pubr() +
  labs(x = NULL, y = "Success (Relative)", title = "Aggregating Models",
       color = "Model Type")

```

<a href="#analyses">Back to top</a>

# Forecasting Accuracy {.tabset .tabset-pills} 

We expect to analyze several attributes of the forecasts collected beyond their contribution to the selecting and aggregating analyses above. 

In the following models, we examine the &#8220;stacking weights forecasts&#8221; and &#8220;horse race forecasts&#8221; separately, though some of the analyses will be conducted in parallel. We note that the stacking weights forecasts allow us to analyze both model-level predictions and forecast exercise-level predictions (i.e., the set of models viewed simultaneously in the stacking weight elicitation). For the horse race forecasts, we will analyze only forecast exercise-level predictions. 

## Correlates of forecast accuracy {.tabset}

We will ask:

  1. What is the average absolute error for the political/socio-economic models? What is it for the epidemiological model? What is it for the Lasso models? These questions will let us gauge whether researchers over- or under- predict the added value of political and socio-economic variables relative to other factors.
  
To assess the average absolute error for the &#8220;stacking weights forecasts&#8221;: for each stacking forecast, we will estimate the stacking weights on an identical subset of the models to those viewed by the forecaster. The average absolute error is calculated as follows:

```{r predict-helpers}
abs_error_stacking <- function(forecast_w){
  # this code assumes that the model names in the forecasting data match those in the dataframe df
  xx <- df[,names(forecast_w)]
  weights <- stack_models(y, xx)
  abs_error <- abs(forecast_w - weights)
  names(abs_error) <- models
  return(abs_error)
}

pretty_print(abs_error_stacking)

```

We will average the resultant absolute errors for each model type (political/socio-economic, Lasso, and epidemiological models):

```{r predict-types}
# Disaggregating by model type
disaggregate_abs_error <- function(forecast_w){
  aae <- abs_error_stacking(forecast_w)
  ind <- ifelse(!names(forecast_w) %in% c("Lasso", "Epi"), "soc_pol", names(forecast_w))
  return(tapply(aae, INDEX = ind, FUN = mean))
}

pretty_print(disaggregate_abs_error)

```

We will then evaluate the average of these average absolute errors across all stacking forecasts.

To assess the average absolute error for the &#8220;horse race forecasts&#8221;: for each horse race forecast, we will examine both (1) the model that was assigned the highest likelihood of being the single best fitting model within the set viewed by the forecaster and (2) the model that was in actuality the single best fitting model among the set of models according to the $R^2$ measure used in Section 2. Our measure of absolute error here is the absolute value of the difference in $R^2$ measures of these models. Obviously, where the forecasters accurately select the correct model, the absolute error is equal to zero. Further, note that this is a forecast-level measure (i.e., we analyze only the top ranked model out of all the models of the horse race). If forecasters assigned equal weights to multiple most-likely models, we will evaluate the average of the differences in $R^2$ between the tied (top) models and the best performing model. 

```{r, warning = F}
abs_error_horserace <- function(r2, top_pred){
  # r2 is a vector of the r2 for each model in the horserace set
  # top_pred is a vector containing the model name(s) of the model predicted to perform the best
  pred <- mean(r2, names(r2) %in% top_pred)
  estimated <- max(r2)
  return(abs(pred-estimated))
}

pretty_print(abs_error_horserace)
```

We did not include a Lasso model within any horse race sets. As such, we can still compare the average absolute errors across political/socio-economic models and epidemiological models. That is, we can compare the average absolute error from the horse race models that predict the epidemiological model will perform the best to those that predict a different model will perform the best. We will not make this comparison unless there are at least 10 models in each category. 


  2. Is the average absolute error of the forecasts correlated with the model performance or the performance of the set of models? That is, are researchers better at forecasting well-performing models or sets of models than poorly-performing models or sets of models? Does the average absolute error of forecasts depend on how differently the models in the set that they saw performed? This question provides some indication of the generalizability of our results in that it shows how much we might expect results to change if we had included better or worse models.

To estimate whether average absolute error of the predicted model weights is correlated with the performance of a model for the &#8220;stacking weights forecasts&#8221;, we will estimate a regression of the form:
  
```{r predict-mod1}
## aae_model is the model-level average absolute error calculated by calc_avg_abs_error
## r2_model is the model r^2 *without* accounting for the Lasso model
## forecast indicates the specific forecasting exercise
quote( lm_robust(aae_model ~ r2_model, clusters = forecast) ) %>% print()
```

We will cluster standard errors at the forecast level. This refers to the seven models on the screen for a single stacking challenge.

To estimate whether the average absolute error for a forecast correlates with the performance of set of models for the &#8220;stacking weights forecasts&#8221;, we will first estimate both the prediction from stacking and the variance of model weights for each subset of models displayed in a stacking exercise using the function `subset_stacking()`:

```{r predict-mod2}

subset_stacking <- function(forecast_w){
  # this code assumes that the model names in the forecasting data match those in the dataframe df
  xx <- df[,names(forecast_w)]
  weights <- stack_models(y, xx)
  var_weights <- var(weights)
  pred <- sum_stack(weights, xx)
  return(c(var_weights, pred))
}

print(subset_stacking)

```

With the estimates from each forecast, we will estimate the regressions:
  
```{r predict-mod3a}
## pred_error is the absolute error of the aggregate model prediction versus the estimated subset stacking model
## var_weights is the variance of weights for the stacking exercise a in a given forecast
## stacking_pred_error is the "r^2" of the stacking prediction
quote( lm_robust(pred_error ~ var_weights)) %>% print()
quote( lm_robust(pred_error ~ stacking_r2) ) %>% print()
```


We will conduct an analogous analysis for the &#8220;horse race forecasts.&#8221; Here, we seek to examine whether the absolute error of the forecasts is correlated with the performance of a model in the horse race forecasts. To do so, we will generate the average absolute error of each horse race forecast (using the same metric as in #1 with the function `abs_error_horserace`). We will regress this measure on two variables. First, we will regress absolute error on the performance $R^2$ of the top-performing model (in actuality). Second, we will regress absolute error on the the variance of the $R^2$ of the models in the set: 

```{r predict-mod3b}
## abs_error_top is a vector of forecast-level errors generated by abs_error_horserace
## r2 top is the best-performing 
## var_fit is the variance of the "r^2" estimates for models included in the horse race forecast
quote( lm_robust(abs_error_top ~ r2_top)) %>% print()
quote( lm_robust(abs_error_top ~ var_fit)) %>% print()
```


  3. Is the average error correlated with logic quality? The quality of the logic of each model was rated ex ante by the research team.
  
We can only assess the correlation between absolute error of the model-level predictions and model quality for the &#8220;stacking weights forecasts&#8221;. To estimate whether the average error of a model is correlated with the logic quality of the model, we will estimate the following model for all the submitted models (i.e., not the epidemiological or Lasso models):
  
```{r predict-mod4}
## logic_quality is a model-level measure of logic quality
quote( lm_robust(aae_model ~ logic_quality, clusters = forecast, subset = !model %in% c("Lasso", "Epi")) ) %>% print()
```

Here, `forecast`, the level of clustering, refers to the seven models on the screen for a single stacking challenge.

  4. How does forecast quality change over time? To answer this question, we will ask forecasters to provide estimates for two different dates and compare accuracy across shorter-term and longer-term forecasts. Again, we will do this separately for the &#8220;stacking weights forecasts&#8221; and the &#8220;horse race forecasts&#8221;.
  
We will make forecast-level comparisons to answer this question. For the &#8220;stacking weights forecasts&#8221;, we will examine the the absolute error of the aggregate model prediction versus the estimated subset stacking model. We will compare average errors (across forecasts) both for the 31 August  2021 and 31 August  2022 predictions. For the &#8220;horse race forecasts&#8221;, we will examine the absolute error in terms of the difference in the $R^2$ of the top predicted versus actual top models in a given forecast. Again, we will compare average errors (across forecasts) at each date. 

<a href="#forecasting-accuracy">Back to top</a>

## Expertise {.tabset}

In addition to assessing the properties of these predictions, we will ask two questions about expertise:

  1. Does increased familiarity with a context lead to better forecasts in that context? Here we will alternatively use two measures of contextual expertise: familiarity with a country and having done research in that country (or cross-national research) before.
  
We will use OLS to regress forecast-level predictive accuracy (defined according to the above measures of absolute error for the stacking and horse race forecasts) on forecaster attributes. We will estimate this regression for each forecast type separately (because the dependent variable absolute error measures are different across the two types of forecasts). Expertise will be measured as an indicator variable for (1) self-reported familiarity with a country or cross-national research; (2) experience doing research in India/Mexico/the US or cross nationally. Both indicators will be coded relative to a specific forecast. These specifications are given by the following code:

```{r expertise}
# expertise will be coded according to two definitions, though the functional form of the regression will be equivalent
quote( lm_robust(pred_error ~ expertise, clusters = forecaster) ) %>% print() # pred_error is the absolute error of a stacking forecast
quote( lm_robust(abs_error_top ~ expertise, clusters = forecaster) ) %>% print() # abs_error_top is the absolute error of the top-rated model in a horse race forecast
```

Here, we will cluster standard errors at the forecaster level. (Note that each response contains least two forecasts per forecaster.) 

  2. Does domain expertise lead to better forecasts? Here we will compare forecasts of faculty members working in public health with forecasts of all other researchers.

As above, we will study this question about forecaster expertise using OLS and analogous specifications to \#1.  The RHS measure of expertise will be an indicator taking the value of 1 if the forecaster works in the field of public health. This variable will be coded at the forecaster (rather than the forecast) level.

If we obtain less than 20 forecasts within any category necessary to answer the above questions (e.g., 10 &#8220;horse race forecasts&#8221; from health economists), we will consider the sample size too small to draw conclusions for that question. We will pool forecasts across the different challenges that people provided responses for (i.e., the global challenge and sub-national US, Mexico, and India challenges).


<a href="#forecasting-accuracy">Back to top</a>

## Overconfidence {.tabset}

Finally, and specific to the &#8220;horse race forecasts&#8221;, we will also examine whether forecasters are overconfident in their predictions. Recall that each forecast will correctly attribute the most weight to the (in actuality) most predictive model or will not correctly do so. We will compare the distribution of probabilities assigned to the top model among forecasts that (1) correctly predicted the top model and (2) incorrectly predicted the top model. We will report the mean of the distribution of probabilities among each subset. This approach is illustrated in the following graph. The vertical lines represent the mean probabilities assigned to the models in each subset.

```{r overconfidence}
# simulated horse race probabilities
data.frame(probs = runif(50)) %>%
  mutate(correct = rbinom(50, size = 1, prob = 0.25 + 0.25 * probs),
         correct_label = ifelse(correct == 1, "Top forecast model\nwas correct", "Top forecast model\nwas incorrect")) %>%
  group_by(correct_label) %>%
  mutate(mean_pred = mean(probs)) %>%
  ggplot(aes(x = probs)) + 
  geom_histogram() +
  facet_grid(correct_label ~.) +
  labs(x = "Probability assigned to top model in horse-race forecast", y = "# Models") +
  geom_vline(aes(xintercept = mean_pred), lwd = 1) +
  theme_minimal() +
  ggpubr::labs_pubr()

```

<a href="#forecasting-accuracy">Back to top</a>

# Extensions {.tabset .tabset-pills} 

## Evolution

We will assess the substantive interpretation of the estimated weights in the stacking model by considering how these weights evolve over time. To do so, we will estimate stacking weights using monthly measures of (logged) cumulative COVID-19 deaths per million (i.e., the measures from January 2021, February 2021, etc.). We will evaluate the function `stack_models()` for each month's outcome and estimate the stacking weights for each model. This will allow us to assess the stability of the estimated weight for each model over time.

## Missingness {.tabset}

For the purposes of evaluating forecasts, we will also estimate variants of the stacking exercise with different assumptions about missingness. Because forecasters are not provided any information about missing data, we aim to ensure that any conclusions that we draw about forecasts are not simply artifacts of our treatment of missing data. To this end, we will estimate the weights with two alternate treatments of the missing data:

1. Omit observations with missingness for any predictor (across models). This reduces our sample size somewhat depending on the scope of missingness, but does not rely on any imputation.
2. Implement multiple imputation on predictors prior to predicting. This will be a data-driven approach that keeps our sample size at its maximum.


### Treatment I

For treatment 1, we use the following code to remove all missing data:

```{r missing-drop}

rm_na <- function(df) {
  na.omit(df)
}

pretty_print(rm_na)

```

And stress test using dummy data with 10\% added missingness (&#8220;Global Challenge, General Models&#8221;) suggests our analyses procedures are not affected by such treatment.  
 
```{r missing-drop-test}

# mwe with global, general challenge

## data
data_aug2020_na <- data_aug2020[[1]]

n_total <- nrow(data_aug2020_na)

n_missing <- 1/10 * n_total # 10% missing

n_col <- length(colnames(data_aug2020_na)) - 1

ind_missing <- vector(mode = "list", length = n_col)
for (i in seq(n_col)) {
  set.seed(i)
  ind_missing[[i]] <- sample(seq(n_total), n_missing)
}
names(ind_missing) <- colnames(data_aug2020_na)[-1]

for (id in names(ind_missing)) {
  id_na <- ind_missing[[id]] 
  data_aug2020_na[[id]][id_na] <- NA
}

## test
data_aug2020_na_drop <- rm_na(data_aug2020_na)
test_dropna <- data.frame(
  Type = c("Gathering", 
           "Selecting - Variance", "Selecting - Stacking",
           "Aggregating - Stacking", "Aggregating - Representative", "Aggregating - Crowd"),
  Result = c(!is.null(gather_r2(data_aug2020_na_drop)), 
             !is.null(select_r2_boot(data_aug2020_na_drop, r=10)), 
             !is.null(select_stack_boot(data_aug2020_na_drop, r=10)),
             !is.null(agg_sumstack_boot(df_w = w_select[[1]], df_mods = data_aug2020_na_drop)),
             !is.null(agg_represent_boot(df_w = data_forecast_w[[1]], df_mods = data_aug2020_na_drop)),
             !is.null(agg_crowd_boot(df_w = data_forecast_w[[1]], df_mods = data_aug2020_na_drop)))
) %>% 
  transform(Result = if_else(Result, "Pass", "Fail"))
test_dropna %>% 
  kable(format = "html", caption = "Stress test results: dropping missing observations", booktabs = TRUE, escape = FALSE) %>% 
  style_kable()

```

<a href="#extensions">Back to top</a>

### Treatment II

For treatment 2, we use the following code for imputing predictors before predicting. We allow the algorithm to choose the best suited method for imputing each affected predictor, and impute any remaining missingness with the mean. 

```{r missing-impute}

# print imputation fn

impute_na <- function(df, ranseed = 1) {
  
  # impute with mice
  pacman::p_load(mice)
  df.imp <- mice::mice(df, seed = ranseed)
  df.out <- mice::complete(df.imp)
  
  # impute remaining na with mean
  if (any(is.na(df.out))) {
    for(i_col in 1:ncol(df.out)) {
      df.out[is.na(df.out[,i_col]), i_col] <- mean(df.out[,i_col], na.rm = TRUE) 
      }
  }
  
  return(df.out)
  
}

pretty_print(impute_na)

```

<a href="#extensions">Back to top</a>


# Appendices {.tabset .tabset-pills}

## Model Submission Platform {#platform}

```{r, appx-platform, fig.show='hold', out.width="50%", fig.cap="Shiny interface: welcome page (left) and variable selection (right)", fig.align="center"}

knitr::include_graphics("figs/shiny_1.png")
knitr::include_graphics("figs/shiny_2.png")

```

<a href="#top">Back to beginning</a>

